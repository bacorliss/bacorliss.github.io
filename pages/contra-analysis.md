## Contra-Analysis: A Holistic Method for Analyzing Effect Size from Controlled Experiments

<img src="https://github.com/bacorliss/bacorliss.github.io/blob/ffb19f44878ad12fcdda860127fbf3c0591680d3/images/project_contra-analysis.PNG?raw=true"/>
<br>
**Project description:** During my postdoctoral research, I developed a new method of data analysis for controlled experiments that can be used to determine if a result has evidence for a meaningful effect size, neglgible effect size, or neither. Additionally, this process can be used to inform the selection of the thresholds for what is meaningful and nigglgible, and visualize effect size across broadly related experiments with different experiment designs, model systems, and interventions that measure the same dependent variable. This process can be used as an alternative to null hypothesis testing p-values.


**Github Repository:** <br>
[https://github.com/bac7wj/contra](https://github.com/bac7wj/contra)
<br><br>

This research is being developed over a series of manuscripts.

------

**Publication:** <br>
B. A. Corliss, Y. Wang, H. Shakeri, P. E. Bourne, **Contra-Analysis: Prioritizing Meaningful Effect Size in Scientific Research** (2022), [doi:10.48550/arXiv.2210.04867](doi:10.48550/arXiv.2210.04867).
<br>

**Abstract:** At every phase of scientific research, scientists must decide how to allocate limited resources to pursue the research inquiries with the greatest potential. This prioritization dictates which controlled interventions are studied, awarded funding, published, reproduced with repeated experiments, investigated in related contexts, and translated for societal use. There are many factors that influence this decision-making, but interventions with larger effect size are often favored because they exert the greatest influence on the system studied. To inform these decisions, scientists must compare effect size across studies with dissimilar experiment designs to identify the interventions with the largest effect. These studies are often only loosely related in nature, using experiments with a combination of different populations, conditions, timepoints, measurement techniques, and experiment models that measure the same phenomenon with a continuous variable. We name this assessment contra-analysis and propose to use credible intervals of the relative difference in means to compare effect size across studies in a meritocracy between competing interventions. We propose a data visualization, the contra plot, that allows scientists to score and rank effect size between studies that measure the same phenomenon, aid in determining an appropriate threshold for meaningful effect, and perform hypothesis tests to determine which interventions have meaningful effect size. We illustrate the use of contra plots with real biomedical research data. Contra-analysis promotes a practical interpretation of effect size and facilitates the prioritization of scientific research.

-------

**Publication:** <br>
B. A. Corliss, Y. Wang, H. Shakeri, P. E. Bourne, **Contra-Analysis for Determining Negligible Effect Size in Scientific Research** (2023), [doi:10.48550/arXiv.2303.09428](doi:10.48550/arXiv.2303.09428).
<br>

**Abstract:** Scientific experiments study interventions that show evidence of an effect size that is meaningfully large, negligibly small, or inconclusively broad. Previously, we proposed contra-analysis as a decision-making process to help determine which interventions have a meaningfully large effect by using contra plots to compare effect size across broadly related experiments. Here, we extend the use of contra plots to determine which results have evidence of negligible (near-zero) effect size. Determining if an effect size is negligible is important for eliminating alternative scientific explanations and identifying approximate independence between an intervention and the variable measured. We illustrate that contra plots can score negligible effect size across studies, inform the selection of a threshold for negligible effect based on broadly related results, and determine which results have evidence of negligible effect with a hypothesis test. No other data visualization can carry out all three of these tasks for analyzing negligible effect size. We demonstrate this analysis technique on real data from biomedical research. This new application of contra plots can differentiate statistically insignificant results with high strength (narrow and near-zero interval estimate of effect size) from those with low strength (broad interval estimate of effect size). Such a designation could help resolve the File Drawer problem in science, where statistically insignificant results are underreported because their interpretation is ambiguous and nonstandard. With our proposed procedure, results designated with negligible effect will be considered strong and publishable evidence of near-zero effect size.

-------
**Publication:** <br>
B. A. Corliss, T. R. Brown, T. Zhang, K. A. Janes, H. Shakeri, P. E. Bourne, **The Most Difference in Means: A Statistic for the Strength of Null and Near-Zero Results** (2022), [doi:10.48550/arXiv.2201.01239](doi:10.48550/arXiv.2201.01239).
<br>

**Abstract:** Statistical insignificance does not suggest the absence of effect, yet scientists must often use null results as evidence of negligible (near-zero) effect size to falsify scientific hypotheses. Doing so must assess a result's null strength, defined as the evidence for a negligible effect size. Such an assessment would differentiate strong null results that suggest a negligible effect size from weak null results that suggest a broad range of potential effect sizes. We propose the most difference in means (δM) as a two-sample statistic that can both quantify null strength and perform a hypothesis test for negligible effect size. To facilitate consensus when interpreting results, our statistic allows scientists to conclude that a result has negligible effect size using different thresholds with no recalculation required. To assist with selecting a threshold, δM can also compare null strength between related results. Both δM and the relative form of δM outperform other candidate statistics in comparing null strength. We compile broadly related results and use the relative δM to compare null strength across different treatments, measurement methods, and experiment models. Reporting the relative δM may provide a technical solution to the file drawer problem by encouraging the publication of null and near-zero results.

-------
**Publication:** <br>
B. A. Corliss, Y. Wang, H. Shakeri, P. E. Bourne, **The Least Difference in Means: A Statistic for Effect Size Strength and Practical Significance** (2022), [doi:10.48550/arXiv.2205.12958](doi:10.48550/arXiv.2205.12958).

<br>

**Abstract:** With limited resources, scientific inquiries must be prioritized for further study, funding, and translation based on their practical significance: whether the effect size is large enough to be meaningful in the real world. Doing so must evaluate a result's effect strength, defined as a conservative assessment of practical significance. We propose the least difference in means (δL) as a two-sample statistic that can quantify effect strength and perform a hypothesis test to determine if a result has a meaningful effect size. To facilitate consensus, δL allows scientists to compare effect strength between related results and choose different thresholds for hypothesis testing without recalculation. Both δL and the relative δL outperform other candidate statistics in identifying results with higher effect strength. We use real data to demonstrate how the relative δL compares effect strength across broadly related experiments. The relative δL can prioritize research based on the strength of their results.

-------

